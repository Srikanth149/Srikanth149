"""
StrOutputParser will enforce the LLM output upon invoke, will retrun to plain string rather than
chat message object generated by LLM model as default which is a String which works fair with langchain compatiblity
this is just to generate summaries,explainations,answers.
you dont care about parsing the output.

Use StructuredOutputParser only when you are sure your LLM will output exactly the
structured schema like JSON,Pydantic models

"""
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
model =ChatOpenAI()
Template1 = PromptTemplate(
    template="write a detail report on the topic: {topic}, stick strict to the topic and avoid irrelavent content",
    input_variables=['topic']
)
Template2= PromptTemplate(
    template='write a 5 line summary on the follwoing text. /n {text}',
    input_variables=['text']
)
parser = StrOutputParser()

chain = Template1|model|parser|Template2|model|parser

result =chain.invoke({"topic": "10 international desserts that look so Indian!"})
print(result)

#Debug
#step1=Template1|model|parser
#step2=Template2|model|parser
#inter=step1.invoke({"topic":"tirupati"})
#print("intermediate output", inter)
#final=step2.invoke({"text":"inter"})
#print("final output", final)